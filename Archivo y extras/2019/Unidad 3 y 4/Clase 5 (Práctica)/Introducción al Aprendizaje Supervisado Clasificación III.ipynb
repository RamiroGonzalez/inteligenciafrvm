{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Aprendizaje Supervisado - Clasificación (Parte III)\n",
    "\n",
    "* Enfoques prácticos para problemas comunes en AS.\n",
    "* Ejemplo de sub-área clásica de IA: Natural Language Processing (NLP)\n",
    "* Conclusiones del aprendizaje supervisado.\n",
    "\n",
    "5to año - Ingeniería en Sistemas de Información\n",
    "\n",
    "Facultad Regional Villa María"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoques prácticos para problemas comunes en Aprendizaje Supervisado\n",
    "\n",
    "#### Outliers\n",
    "\n",
    "* Un _outlier_ es un punto que presenta una anomalía con respecto a nuestros demás datos. En la regresión, se considera también como outliers a aquellos puntos muy específicos para los cuales nuestra predicción $\\hat{y}_i$ se encuentra muy lejos del valor real $y_i$.\n",
    "\n",
    "![](images/outlier.jpg)\n",
    "\n",
    "Fuente: https://statsland.wordpress.com/2012/09/24/outliers-are-they-good-or-bad/\n",
    "\n",
    "* Tales puntos suelen ser particularmente molestos, ya que no podemos explicar por qué la predicción está tan lejos, y al ser muy baja su cantidad no afectan demasiado el error global, por lo que plantean la duda sobre qué hacer con ellos.\n",
    "\n",
    "* Un enfoque común (pero muchas veces incorrecto) es asumir que fueron producto de un error en la toma de datos o como un caso extremadamente poco probable y que no representa en absoluto a los demás datos. A partir de esa asunción se suele proceder a ignorarlo o eliminarlo (si afecta el error global o alguna métrica de forma considerable, y consideramos que no representa un caso que nuestros datos estén modelando).\n",
    "\n",
    "* De alguna forma los outliers tienen que ser considerados y en lo posible investigados (y como mínimo tener el registro de que ocurrieron); pues a menudo suelen indicar que existe un aspecto en los datos que nuestro modelo del problema no está considerando, por ejemplo, la falta un feature que considere algún punto de vista de los datos no tenido en cuenta previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción multi-label\n",
    "\n",
    "* Para algunos datasets, las observaciones están etiquetadas con más de una salida, las cuáles no son excluyentes entre sí.\n",
    "\n",
    "* Un enfoque común para estos casos es utilizar un predictor (regresor o clasificador) por cada uno de los labels. En el caso de la clasificación, este enfoque consiste en utilizar un clasificador binario OneVsRest para cada una de las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset con información faltante\n",
    "\n",
    "* Existen casos donde los datasets no contienen información para todos sus features. Para estos casos suelen tomarse dos enfoques. Uno es eliminar las observaciones afectadas del dataset. \n",
    "\n",
    "* Otro enfoque consiste en utilizar un predictor (ej Random Forest) para estimar, en base a las demás observaciones que contienen valor en el feature, cuál es el valor que podría tener ese feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of Dimensionality\n",
    "\n",
    "* _The Curse of Dimensionality_ (Bellman, 1957) se refiere al problema donde, a medida que crece linealmente la cantidad de dimensiones de nuestros datos, la complejidad inherente de procesarlas crece a la vez en un orden exponencial.\n",
    "\n",
    "* En ML, esto tiene dos consecuencias principales. La primera es que a medida que aumentan los features (es decir la *dimensión* $d$ del dataset, se necesitan cada vez más datos para tener una muestra representativa de los mismos que abarque una parte significativa de las combinaciones de todos los features.\n",
    "\n",
    "* La segunda consecuencia es que, al existir tantas combinaciones de los features, pasa a haber una enorme cantidad de regiones distintas en la función que intentamos aproximar, por lo que muchos métodos no pueden capturar la forma de una función tan compleja.\n",
    "\n",
    "* Una forma de mitigarlo es mediante **_Principal Components Analysis_** (PCA), que nos ayuda a reducir la dimensionalidad de nuestro dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No free lunch theorem\n",
    "\n",
    "    \"All models are wrong, but some models are useful.\"\n",
    "\n",
    "                         — George Box (Box and Draper 1987, p424)\n",
    "                         \n",
    "*No free lunch theorem* (Wolpert, 1996) establece que no existe un modelo universalmente mejor a todos los demás, sino que cada modelo, al partir de diversas asunciones, tiene sus ventajas y desventajas. Esto puede hacer que un modelo desempeñarse muy bien en un cierto dominio y muy pobremente en otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occam's Razor\n",
    "\n",
    "    “When presented with competing hypothetical answers to a problem, one should select the one that makes the fewest assumptions”\n",
    "\n",
    "En ML puede interpretarse de varias formas, una de las cuáles establece que, ante distintos modelos igualmente competitivos para un determinado problema, debemos optar por el más simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persistencia de un modelo en scikit-learn\n",
    "\n",
    "Para guardar los resultados del entrenamiento de un modelo predictivo y no tener que realizar todo el entrenamiento cada vez que se desee usar el modelo en el futuro, scikit-learn provee la librería *joblib*, que es una extensión de la librería nativa *pickle* adaptada a un guardado más eficiente de objetos que contengan *ndarrays*. y que permite persistir el entrenamiento de un modelo en disco y cargarlo nuevamente más tarde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # persistimos un modelo creado con sklearn (ejemplo: clasificador_svm = svm.SVC())\n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(clasificador_svm, 'clasificador.pkl')\n",
    "\n",
    "    # cargamos el modelo persistido en memoria\n",
    "    clf = joblib.load('clasificador.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más información aquí: http://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Hasta recién hemos visto dominios de datos **estructurados**, es decir dominios en donde los datos siguen una estructura definida, típicamente porque los mismos han sido recolectados de acuerdo a un modelo que sigue esa misma estructura.\n",
    "\n",
    "No obstante, muchos de los datos generados por sistemas de información no están estructurados de acuerdo a un modelo. Estos datos se conocen como datos **no estructurados** y, como tales, entrenar un modelo sobre ellos no suele ser directo, sino que, por ejemplo, se debe hacer un pre-procesamiento particular de los datos de acuerdo al dominio del problema. A continuación vamos a mostrar brevemente una de las sub-áreas de IA que trabaja con una gran parte de los datos no estructurados: Procesamiento del Lenguaje Natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de sub-área clásica de IA: Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "El procesamiento de lenguaje natural es una de las sub-áreas clásicas de la IA, junto con el procesamiento de imágenes/video, procesamiento de voz o la robótica.\n",
    "\n",
    "Como cada sub-área concreta que lleva años siendo estudiada, tiene consideraciones y técnicas particulares que suelen ser aplicables sólo en la misma.\n",
    "\n",
    "En particular, NLP apunta toma un texto como entrada y genera una salida relevante como ser el texto traducido, la identificación de la categoría a la que pertenece, análisis de sentimientos, entre otros.\n",
    "\n",
    "Al trabajar con datos no estructurados en forma de texto de longitud variable con sus posibles ambigüedades, errores y demás, no suele ser posible entrenar directamente un modelo de predicción sobre ellos (por ejemplo, porque los features directamente no están definidos). A continuación podemos ver una vista general de la construcción de un sistema de clasificación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/building_text_classification_system.png)\n",
    "\n",
    "Fuente: Figura 4.2 de Text Analytics with Python: A Practical Real-World Approach to Gaining Actionable Insights from Your Data (D. Sarkar, 2016). Buen libro para aquellos que quieran profundizar en NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial recomendado para iniciarse en NLP: [Working with Text Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra librería recomendadada para NLP: [Natural Language Toolkit (NLTK)](http://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curso recomendado para aquellos que desean profundizar en NLP: Procesamiento de Lenguaje Natural, dictado en la Facultad de Matemática, Astronomía y Física, UNC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones de la Introducción al Aprendizaje Supervisado\n",
    "\n",
    "* El aprendizaje supervisado permite predecir salidas de una función desconocida $f(X)$ al tomarla como una caja negra para entradas $X$ no observadas, dado un entrenamiento previo con $(X, f(X))$ conocidos.\n",
    "\n",
    "* Sus técnicas permiten obtener un gran tasa de aciertos con métodos de variada complejidad para un rango muy importante de problemas, en campos diversos como el reconocimiento de imágenes, robótica, procesamiento de texto, entre otros.\n",
    "\n",
    "* En las clases hasta aquí se mostró una introducción al aprendizaje supervisado, mostrando cuáles son sus principales características, modelos y cómo evaluarlos.\n",
    "\n",
    "* Debido a que el campo es muy amplio, muchos modelos han quedado fuera del alcance de estas clases; no obstante confiamos que al conocer las bases y al haber implementado varios, el aprendizaje de nuevas técnicas no será dificultoso puesto que la gran mayoría se como una extensión de lo visto en estas clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El aprendizaje supervisado es, dentro de machine learning, el área con más aplicaciones y avances científicos. No obstante, al igual que como sucede en todo el campo de ML, es en este momento \"más un arte que una ciencia\", debido a que las soluciones dependen mucho de la experiencia previa con la que uno cuente, sumado a la necesidad de conocer a fondo el dataset con el que se trabaja, lo que hace que no exista una metodología estándar para encarar o evaluar muchos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/machine_learning_system.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: \n",
    "\n",
    "1. [Debate reciente muy interesante sobre la \"objetividad\" y los sesgos peligrosos en modelos de ML](https://old.reddit.com/r/MachineLearning/comments/bhm0si/d_everyone_building_machine_learning_products_has/)\n",
    "\n",
    "\n",
    "Algunos frameworks poderosos que usan *gradient boosting* en árboles:\n",
    "\n",
    "1. https://github.com/dmlc/xgboost\n",
    "2. https://catboost.ai/\n",
    "3. https://github.com/Microsoft/LightGBM\n",
    "\n",
    "Para tener en cuenta en la resolución del TP:\n",
    "\n",
    "1. [Notebook con un resumen rápido de algunos modelos de clasificación](https://github.com/inteligenciafrvm/inteligenciafrvm/blob/master/Material%20Extra/Clasificaci%C3%B3n%20-%20Vista%20general%20de%20algunos%20modelos/Clasificaci%C3%B3n%20-%20Vista%20general%20de%20algunos%20modelos.ipynb).\n",
    "2. Algunos usos básicos de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         c         d\n",
      "0  0.407687  0.055366  0.788535  0.287305\n",
      "1  0.450351  0.303912  0.526400  0.623812\n",
      "2  0.776775  0.686242  0.980939  0.600816\n",
      "3  0.813969  0.708645  0.027535  0.904267\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "df = pd.DataFrame(np.random.rand(4,4), columns = list('abcd'))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.407687\n",
      "1    0.450351\n",
      "2    0.776775\n",
      "3    0.813969\n",
      "Name: a, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# seleccionar columna 'a'\n",
    "print(df['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    0.776775\n",
       "b    0.686242\n",
       "c    0.980939\n",
       "d    0.600816\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seleccionar fila 2, todas las columnas\n",
    "df.loc[2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9809388631878051"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seleccionar fila 2, solo columna c\n",
    "df.loc[2, 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         c         d\n",
      "0  0.407687  0.055366  0.788535  0.287305\n",
      "1  0.450351  0.303912  0.526400  3.623812\n",
      "2  0.776775  0.686242  0.980939  3.600816\n",
      "3  0.813969  0.708645  0.027535  0.904267\n"
     ]
    }
   ],
   "source": [
    "# sumarle el valor 3 a las filas 2 y 3 de la columna d\n",
    "df.loc[1:2, 'd'] += 3\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       a      b      c      d\n",
      "0  False  False  False  False\n",
      "1  False  False  False   True\n",
      "2  False  False  False   True\n",
      "3  False  False  False  False\n"
     ]
    }
   ],
   "source": [
    "# crear una mascara para aplicar en valores que cumplan una \n",
    "# cierta condicion, por ejemplo a aquellos valores mayores \n",
    "# a 1 de todas las columnas (se podria filtrar tambien por\n",
    "# cada columna)\n",
    "\n",
    "mask = df > 1\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         c         d\n",
      "0  0.407687  0.055366  0.788535  0.287305\n",
      "1  0.450351  0.303912  0.526400  4.623812\n",
      "2  0.776775  0.686242  0.980939  4.600816\n",
      "3  0.813969  0.708645  0.027535  0.904267\n"
     ]
    }
   ],
   "source": [
    "# con esta mascara, vamos a aplicar una una operación solamente a \n",
    "# aquellos datos que cumplen con su condicion\n",
    "\n",
    "df[mask] = df[mask] + 1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       a      b      c      d\n",
      "0  False   True  False   True\n",
      "1  False  False  False   True\n",
      "2  False  False  False   True\n",
      "3  False  False   True  False\n"
     ]
    }
   ],
   "source": [
    "# si quisieramos hacer una máscara para más de una condicion...\n",
    "\n",
    "# aplicamos la mascara a los datos mayores a 1 o menores a 0.3\n",
    "mask = (df > 1) | (df < 0.3)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         c         d    e\n",
      "0  0.407687  0.055366  0.788535  0.287305  0.0\n",
      "1  0.450351  0.303912  0.526400  4.623812  0.0\n",
      "2  0.776775  0.686242  0.980939  4.600816  0.0\n",
      "3  0.813969  0.708645  0.027535  0.904267  0.0\n"
     ]
    }
   ],
   "source": [
    "# crear una nueva columna con todos sus valores en 0\n",
    "df['e'] = pd.Series(np.zeros(4))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         d    e\n",
      "0  0.407687  0.055366  0.287305  0.0\n",
      "1  0.450351  0.303912  4.623812  0.0\n",
      "2  0.776775  0.686242  4.600816  0.0\n",
      "3  0.813969  0.708645  0.904267  0.0\n"
     ]
    }
   ],
   "source": [
    "# eliminar una columna\n",
    "df = df.drop(columns=['c'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         d    e         f\n",
      "0  0.407687  0.055366  0.287305  0.0  0.015907\n",
      "1  0.450351  0.303912  4.623812  0.0  1.405233\n",
      "2  0.776775  0.686242  4.600816  0.0  3.157272\n",
      "3  0.813969  0.708645  0.904267  0.0  0.640805\n"
     ]
    }
   ],
   "source": [
    "# crear una nueva columna producto de multiplicar columnas b y d\n",
    "df['f'] = df['b'] * df['d']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         d    e         f\n",
      "0  0.407687  0.055366  0.287305  0.0  0.015907\n",
      "1       NaN  0.303912  4.623812  0.0  1.405233\n",
      "2  0.776775  0.686242  4.600816  0.0  3.157272\n",
      "3  0.813969  0.708645  0.904267  0.0  0.640805\n"
     ]
    }
   ],
   "source": [
    "# alguien perdio un dato en la columna a!\n",
    "df.loc[1, 'a'] = np.nan\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         d    e         f\n",
      "0  0.407687  0.055366  0.287305  0.0  0.015907\n",
      "1  0.666144  0.303912  4.623812  0.0  1.405233\n",
      "2  0.776775  0.686242  4.600816  0.0  3.157272\n",
      "3  0.813969  0.708645  0.904267  0.0  0.640805\n"
     ]
    }
   ],
   "source": [
    "# podemos completar este dato con el promedio de la columna\n",
    "df = df.fillna(df['a'].mean())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         d    e         f      g\n",
      "0  0.407687  0.055366  0.287305  0.0  0.015907   rojo\n",
      "1  0.666144  0.303912  4.623812  0.0  1.405233   rojo\n",
      "2  0.776775  0.686242  4.600816  0.0  3.157272    NaN\n",
      "3  0.813969  0.708645  0.904267  0.0  0.640805  verde\n"
     ]
    }
   ],
   "source": [
    "# para valores categóricos podemos completar los datos faltantes usando la moda...\n",
    "df['g'] = ['rojo', 'rojo', np.nan, 'verde']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         d    e         f      g\n",
      "0  0.407687  0.055366  0.287305  0.0  0.015907   rojo\n",
      "1  0.666144  0.303912  4.623812  0.0  1.405233   rojo\n",
      "2  0.776775  0.686242  4.600816  0.0  3.157272   rojo\n",
      "3  0.813969  0.708645  0.904267  0.0  0.640805  verde\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna(df['g'].mode()[0])  \n",
    "# notar que se usa el subindice 0 porque el objeto resultante es de tipo Series con un elemento\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df['g'].mode()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
